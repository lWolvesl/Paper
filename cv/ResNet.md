# Deep Residual Learning for Image Recognition

[paper](https://arxiv.org/abs/1512.03385)

## 1. abstract
- 开篇指出深层神经网络难以训练
- 提出对传统直接学习映射的改进，让堆叠的非线性层去拟合另一个映射，继而解决了深层网络的训练问题
- 评估了高达152层的残差网络，并在多个数据集上取得第一

## 2. conclusion
404

## 3. Reading
- instruction
  - 指出近期的证据表明深层网络在许多任务上表现更佳，但同时更深的网络的错误率也更高了
  - 介绍原因：梯度爆炸/消失（notorious 臭名昭著）
  - 之前的解决方案如归一化，初始化，batch normalization，都只能缓解，但无法根本解决，并且理论上较深的模型不应该比浅层模型有更高的训练误差
  - 提出了深度残差学习框架，在理论上可以解决梯度爆炸/消失问题，并且可以简化训练过程
  - $F(x) + x$ 公式代表了残差连接，可以跳过多个层，并且既不增加参数，也没有增加计算的复杂度
  - 新的框架更容易优化，并且可以从深层网络中提取出更丰富的特征，比传统的plain nets更好
  - 在多个数据集上取得了第一
- related work
  - VLAD 聚类->残差->串联->向量
  - 捷径连接 残差链接，跨层传递激活值，解决梯度消失/爆炸
  - 残差块无多余参数，计算复杂度低
  - highway networks 使用门控机制，控制信息流动,但是深层网络并没有提升精度
    - 门控需要额外的参数，计算复杂
- deep residual learning
  - 提出了深度残差学习框架，进行假设，通过实验证明获得了合理的处理
  - 解决恒等映射，由于使用的连接无参数所有没有额外的计算复杂度（说过好几次了），但是要解决维度匹配的问题
    - $y=\mathcal{F}\left(x,\left\{W_{i}\right\}\right)+W_{s} x . (2)$
    - 公式2中，$F(x)$ 是残差函数，$W_s$ 是恒等映射的权重，$x$ 是输入，$y$ 是输出
    - 不仅适用于全连接，也适用于卷积
- network architectures
  - 实现为等维度连接，虚线表示维度不匹配，需要进行维度匹配
  - 维度不匹配采用1x1卷积核进行匹配（增加/减少通道数），步长为2的卷积或池化层（空间尺寸）
  - 如果特征图大小相同，则使用同等数量的卷积核
  - 如果特征图大小减半，则卷积核数量增加一倍
  - 直接通过stride=2的卷积层进行下采样（减少计算量、扩大感受野、提取高层次特征等）
    - 降低分辨率的同时，增加卷积核数量，这样保持了信息的容量，并且减少了计算量
    - 这是网络设计的核心策略之一，在卷积层使用(跳跃连接使用步长为2的卷积核，但是不称作为下采样)
  - Implementation
    - 使用了BN(Batch Normalization)，在每个卷积核后，激活函数之前
    - 动态调整学习率，初试为0.1，当模型到达error plateaus时，将学习率除以10
      - 前期快速收敛，后期稳定收敛
    - 使用随机梯度下降(SGD),使用 0.0001 的权重衰减和 0.9 的动量
    - 最多进行了60万次迭代，未使用dropout

## 4. Try some work
### 4.1 what does the author try to accomplish?
- 解决深层神经网络训练难题，解决梯度消失/爆炸，让深层网络更容易训练
### 4.2 what were the key element of the approach?
- 提出了残差网络框架，通过残差连接，将前面某一层的激活值跳过一层或多层传到后面，与主分支的输出相加，使得后层可以残差函数，使梯度更容易回传，以得到有效训练
```text
输入x → 主分支（卷积层、激活函数等） → 输出F(x)  
└────────────── 跳跃连接 → 输出y = x + F(x)
```
- 传递恒等映射（或线性投影），确保输入与主分支输出维度匹配。当通道数或尺寸变化时，使用**1x1卷积（步长=2）**调整输入。
### 4.3 what can you use in your work?
- 404
### 4.4 what other reference do you want to follow?
- Densely Connected Convolutional Networks